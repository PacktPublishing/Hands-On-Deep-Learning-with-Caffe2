{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Deep Q network to play in an OpenAI Gym environment for cartpole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies to run the reinforcement experiments using Caffe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import core\n",
    "from ml.rl.test.gym.gym_predictor import (\n",
    "    GymDDPGPredictor,\n",
    "    GymDQNPredictor,\n",
    "    GymDQNPredictorPytorch,\n",
    ")\n",
    "from ml.rl.test.gym.open_ai_gym_environment import (\n",
    "    EnvType,\n",
    "    ModelType,\n",
    "    OpenAIGymEnvironment,\n",
    ")\n",
    "from ml.rl.test.utils import write_lists_to_csv\n",
    "from ml.rl.thrift.core.ttypes import (\n",
    "    CNNParameters,\n",
    "    ContinuousActionModelParameters,\n",
    "    DDPGModelParameters,\n",
    "    DDPGNetworkParameters,\n",
    "    DDPGTrainingParameters,\n",
    "    DiscreteActionModelParameters,\n",
    "    KnnParameters,\n",
    "    RLParameters,\n",
    "    TrainingParameters,\n",
    ")\n",
    "from ml.rl.training.continuous_action_dqn_trainer import ContinuousActionDQNTrainer\n",
    "from ml.rl.training.ddpg_trainer import DDPGTrainer\n",
    "from ml.rl.training.discrete_action_trainer import DiscreteActionTrainer\n",
    "from ml.rl.training.dqn_trainer import DQNTrainer\n",
    "from ml.rl.training.parametric_dqn_trainer import ParametricDQNTrainer\n",
    "from ml.rl.training.rl_dataset import RLDataset\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the possible next actions function from gym_env gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_next_actions(gym_env, model_type, terminal):\n",
    "    if model_type in (\n",
    "        ModelType.DISCRETE_ACTION.value,\n",
    "        ModelType.PYTORCH_DISCRETE_DQN.value,\n",
    "    ):\n",
    "        possible_next_actions = [\n",
    "            0 if terminal else 1 for __ in range(gym_env.action_dim)\n",
    "        ]\n",
    "        possible_next_actions_lengths = gym_env.action_dim\n",
    "    elif model_type in (\n",
    "        ModelType.PARAMETRIC_ACTION.value,\n",
    "        ModelType.PYTORCH_PARAMETRIC_DQN.value,\n",
    "    ):\n",
    "        if terminal:\n",
    "            possible_next_actions = np.array([])\n",
    "            possible_next_actions_lengths = 0\n",
    "        else:\n",
    "            possible_next_actions = np.eye(gym_env.action_dim)\n",
    "            possible_next_actions_lengths = gym_env.action_dim\n",
    "    elif model_type == ModelType.CONTINUOUS_ACTION.value:\n",
    "        possible_next_actions = None\n",
    "        possible_next_actions_lengths = 0\n",
    "    return possible_next_actions, possible_next_actions_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training using caffe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    c2_device,\n",
    "    gym_env,\n",
    "    model_type,\n",
    "    trainer,\n",
    "    test_run_name,\n",
    "    score_bar,\n",
    "    num_episodes=100,#301,\n",
    "    max_steps=5000,\n",
    "    train_every_ts=100,\n",
    "    train_after_ts=10,\n",
    "    test_every_ts=100,\n",
    "    test_after_ts=10,\n",
    "    num_train_batches=1,\n",
    "    avg_over_num_episodes=100,\n",
    "    render=True,\n",
    "    save_timesteps_to_dataset=None,\n",
    "    start_saving_from_episode=0,\n",
    "    batch_rl_file_path=None,\n",
    "):\n",
    "\n",
    "    if model_type == ModelType.CONTINUOUS_ACTION.value:\n",
    "        predictor = GymDDPGPredictor(trainer)\n",
    "    elif model_type in (\n",
    "        ModelType.PYTORCH_DISCRETE_DQN.value,\n",
    "        ModelType.PYTORCH_PARAMETRIC_DQN.value,\n",
    "    ):\n",
    "        predictor = GymDQNPredictorPytorch(trainer)\n",
    "    else:\n",
    "        predictor = GymDQNPredictor(trainer, c2_device)\n",
    "\n",
    "    if batch_rl_file_path is not None:\n",
    "        return train_gym_batch_rl(\n",
    "            model_type,\n",
    "            trainer,\n",
    "            predictor,\n",
    "            batch_rl_file_path,\n",
    "            gym_env,\n",
    "            num_train_batches,\n",
    "            test_every_ts,\n",
    "            test_after_ts,\n",
    "            avg_over_num_episodes,\n",
    "            score_bar,\n",
    "            test_run_name,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return train_gym_online_rl(\n",
    "            c2_device,\n",
    "            gym_env,\n",
    "            model_type,\n",
    "            trainer,\n",
    "            predictor,\n",
    "            test_run_name,\n",
    "            score_bar,\n",
    "            num_episodes,\n",
    "            max_steps,\n",
    "            train_every_ts,\n",
    "            train_after_ts,\n",
    "            test_every_ts,\n",
    "            test_after_ts,\n",
    "            num_train_batches,\n",
    "            avg_over_num_episodes,\n",
    "            render,\n",
    "            save_timesteps_to_dataset,\n",
    "            start_saving_from_episode,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train off of fixed set of stored transitions generated off-policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gym_batch_rl(\n",
    "    model_type,\n",
    "    trainer,\n",
    "    predictor,\n",
    "    batch_rl_file_path,\n",
    "    gym_env,\n",
    "    num_train_batches,\n",
    "    test_every_ts,\n",
    "    test_after_ts,\n",
    "    avg_over_num_episodes,\n",
    "    score_bar,\n",
    "    test_run_name,\n",
    "):\n",
    "    \"\"\"Train off of fixed set of stored transitions generated off-policy.\"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    avg_reward_history, timestep_history = [], []\n",
    "\n",
    "    batch_dataset = RLDataset(batch_rl_file_path)\n",
    "    batch_dataset.load()\n",
    "    gym_env.replay_memory = batch_dataset.replay_memory\n",
    "    test_every_ts_n = 1\n",
    "\n",
    "    for _ in range(num_train_batches):\n",
    "        samples = gym_env.sample_memories(trainer.minibatch_size, model_type)\n",
    "        trainer.train(samples)\n",
    "        total_timesteps += trainer.minibatch_size\n",
    "\n",
    "        # Evaluation loop\n",
    "        if (\n",
    "            total_timesteps > (test_every_ts * test_every_ts_n)\n",
    "            and total_timesteps > test_after_ts\n",
    "        ):\n",
    "            avg_rewards, avg_discounted_rewards = gym_env.run_ep_n_times(\n",
    "                avg_over_num_episodes, predictor, test=True\n",
    "            )\n",
    "            avg_reward_history.append(avg_rewards)\n",
    "            timestep_history.append(total_timesteps)\n",
    "            logger.info(\n",
    "                \"Achieved an average reward score of {} over {} evaluations.\"\n",
    "                \" Total timesteps: {}.\".format(\n",
    "                    avg_rewards, avg_over_num_episodes, total_timesteps\n",
    "                )\n",
    "            )\n",
    "            test_every_ts_n += 1\n",
    "\n",
    "            if score_bar is not None and avg_rewards > score_bar:\n",
    "                logger.info(\n",
    "                    \"Avg. reward history for {}: {}\".format(\n",
    "                        test_run_name, avg_reward_history\n",
    "                    )\n",
    "                )\n",
    "                return avg_reward_history, trainer, predictor\n",
    "\n",
    "    # Always eval after last training batch\n",
    "    avg_rewards, avg_discounted_rewards = gym_env.run_ep_n_times(\n",
    "        avg_over_num_episodes, predictor, test=True\n",
    "    )\n",
    "    avg_reward_history.append(avg_rewards)\n",
    "    timestep_history.append(total_timesteps)\n",
    "    logger.info(\n",
    "        \"Achieved an average reward score of {} over {} evaluations.\"\n",
    "        \" Total timesteps: {}.\".format(\n",
    "            avg_rewards, avg_over_num_episodes, total_timesteps\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Avg. reward history for {}: {}\".format(test_run_name, avg_reward_history)\n",
    "    )\n",
    "    return avg_reward_history, timestep_history, trainer, predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train off of dynamic set of transitions generated on-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gym_online_rl(\n",
    "    c2_device,\n",
    "    gym_env,\n",
    "    model_type,\n",
    "    trainer,\n",
    "    predictor,\n",
    "    test_run_name,\n",
    "    score_bar,\n",
    "    num_episodes,\n",
    "    max_steps,\n",
    "    train_every_ts,\n",
    "    train_after_ts,\n",
    "    test_every_ts,\n",
    "    test_after_ts,\n",
    "    num_train_batches,\n",
    "    avg_over_num_episodes,\n",
    "    render,\n",
    "    save_timesteps_to_dataset,\n",
    "    start_saving_from_episode,\n",
    "):\n",
    "    \"\"\"Train off of dynamic set of transitions generated on-policy.\"\"\"\n",
    "\n",
    "    total_timesteps = 0\n",
    "    avg_reward_history, timestep_history = [], []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        terminal = False\n",
    "        next_state = gym_env.transform_state(gym_env.env.reset())\n",
    "        next_action = gym_env.policy(predictor, next_state, False)\n",
    "        reward_sum = 0\n",
    "        ep_timesteps = 0\n",
    "\n",
    "        if model_type == ModelType.CONTINUOUS_ACTION.value:\n",
    "            trainer.noise.clear()\n",
    "\n",
    "        while not terminal:\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            if render:\n",
    "                gym_env.env.render()\n",
    "\n",
    "            if gym_env.action_type == EnvType.DISCRETE_ACTION:\n",
    "                action_index = np.argmax(action)\n",
    "                next_state, reward, terminal, _ = gym_env.env.step(action_index)\n",
    "            else:\n",
    "                next_state, reward, terminal, _ = gym_env.env.step(action)\n",
    "            next_state = gym_env.transform_state(next_state)\n",
    "\n",
    "            ep_timesteps += 1\n",
    "            total_timesteps += 1\n",
    "            next_action = gym_env.policy(predictor, next_state, False)\n",
    "            reward_sum += reward\n",
    "\n",
    "            (\n",
    "                possible_next_actions,\n",
    "                possible_next_actions_lengths,\n",
    "            ) = get_possible_next_actions(gym_env, model_type, terminal)\n",
    "\n",
    "            gym_env.insert_into_memory(\n",
    "                np.float32(state),\n",
    "                action,\n",
    "                np.float32(reward),\n",
    "                np.float32(next_state),\n",
    "                next_action,\n",
    "                terminal,\n",
    "                possible_next_actions,\n",
    "                possible_next_actions_lengths,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "            if save_timesteps_to_dataset and i >= start_saving_from_episode:\n",
    "                save_timesteps_to_dataset.insert(\n",
    "                    state.tolist(),\n",
    "                    action.tolist(),\n",
    "                    reward,\n",
    "                    next_state.tolist(),\n",
    "                    next_action.tolist(),\n",
    "                    terminal,\n",
    "                    possible_next_actions,\n",
    "                    possible_next_actions_lengths,\n",
    "                    1,\n",
    "                )\n",
    "\n",
    "            # Training loop\n",
    "            if (\n",
    "                total_timesteps % train_every_ts == 0\n",
    "                and total_timesteps > train_after_ts\n",
    "                and len(gym_env.replay_memory) >= trainer.minibatch_size\n",
    "            ):\n",
    "                for _ in range(num_train_batches):\n",
    "                    if model_type in (\n",
    "                        ModelType.CONTINUOUS_ACTION.value,\n",
    "                        ModelType.PYTORCH_DISCRETE_DQN.value,\n",
    "                        ModelType.PYTORCH_PARAMETRIC_DQN.value,\n",
    "                    ):\n",
    "                        samples = gym_env.sample_memories(\n",
    "                            trainer.minibatch_size, model_type\n",
    "                        )\n",
    "                        trainer.train(samples)\n",
    "                    else:\n",
    "                        with core.DeviceScope(c2_device):\n",
    "                            gym_env.sample_and_load_training_data_c2(\n",
    "                                trainer.minibatch_size, model_type\n",
    "                            )\n",
    "                            trainer.train()\n",
    "\n",
    "            # Evaluation loop\n",
    "            if total_timesteps % test_every_ts == 0 and total_timesteps > test_after_ts:\n",
    "                avg_rewards, avg_discounted_rewards = gym_env.run_ep_n_times(\n",
    "                    avg_over_num_episodes, predictor, test=True\n",
    "                )\n",
    "                avg_reward_history.append(avg_rewards)\n",
    "                timestep_history.append(total_timesteps)\n",
    "                logger.info(\n",
    "                    \"Achieved an average reward score of {} over {} evaluations.\"\n",
    "                    \" Total episodes: {}, total timesteps: {}.\".format(\n",
    "                        avg_rewards, avg_over_num_episodes, i + 1, total_timesteps\n",
    "                    )\n",
    "                )\n",
    "                if score_bar is not None and avg_rewards > score_bar:\n",
    "                    logger.info(\n",
    "                        \"Avg. reward history for {}: {}\".format(\n",
    "                            test_run_name, avg_reward_history\n",
    "                        )\n",
    "                    )\n",
    "                    return avg_reward_history, trainer, predictor\n",
    "\n",
    "            if max_steps and ep_timesteps >= max_steps:\n",
    "                break\n",
    "\n",
    "        # Always eval on last episode if previous eval loop didn't return.\n",
    "        if i == num_episodes - 1:\n",
    "            avg_rewards, avg_discounted_rewards = gym_env.run_ep_n_times(\n",
    "                avg_over_num_episodes, predictor, test=True\n",
    "            )\n",
    "            avg_reward_history.append(avg_rewards)\n",
    "            timestep_history.append(total_timesteps)\n",
    "            logger.info(\n",
    "                \"Achieved an average reward score of {} over {} evaluations.\"\n",
    "                \" Total episodes: {}, total timesteps: {}.\".format(\n",
    "                    avg_rewards, avg_over_num_episodes, i + 1, total_timesteps\n",
    "                )\n",
    "            )\n",
    "\n",
    "    logger.info(\n",
    "        \"Avg. reward history for {}: {}\".format(test_run_name, avg_reward_history)\n",
    "    )\n",
    "    return avg_reward_history, timestep_history, trainer, predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the gym training using Caffe2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gym(params,\n",
    "            score_bar,\n",
    "            gpu_id,\n",
    "            save_timesteps_to_dataset=None,\n",
    "            start_saving_from_episode=0,\n",
    "            batch_rl_file_path=None):\n",
    "# Caffe2 core uses the min of caffe2_log_level and minloglevel\n",
    "# to determine loglevel. See caffe2/caffe2/core/logging.cc for more info.\n",
    "    core.GlobalInit([\"caffe2\", \"--caffe2_log_level=2\", \"--minloglevel=2\"])\n",
    "\n",
    "    logger.info(\"Running gym with params\")\n",
    "    logger.info(params)\n",
    "    rl_parameters = RLParameters(**params[\"rl\"])\n",
    "\n",
    "    env_type = params[\"env\"]\n",
    "    env = OpenAIGymEnvironment(\n",
    "        env_type,\n",
    "        rl_parameters.epsilon,\n",
    "        rl_parameters.softmax_policy,\n",
    "        params[\"max_replay_memory_size\"],\n",
    "        rl_parameters.gamma,\n",
    "    )\n",
    "    model_type = params[\"model_type\"]\n",
    "    c2_device = core.DeviceOption(\n",
    "        caffe2_pb2.CPU if gpu_id == USE_CPU else caffe2_pb2.CUDA, gpu_id\n",
    "    )\n",
    "    use_gpu = gpu_id != USE_CPU\n",
    "\n",
    "    if model_type == ModelType.PYTORCH_DISCRETE_DQN.value:\n",
    "        training_settings = params[\"training\"]\n",
    "        training_parameters = TrainingParameters(**training_settings)\n",
    "        if env.img:\n",
    "            assert (\n",
    "                training_parameters.cnn_parameters is not None\n",
    "            ), \"Missing CNN parameters for image input\"\n",
    "            training_parameters.cnn_parameters = CNNParameters(\n",
    "                **training_settings[\"cnn_parameters\"]\n",
    "            )\n",
    "            training_parameters.cnn_parameters.conv_dims[0] = env.num_input_channels\n",
    "            training_parameters.cnn_parameters.input_height = env.height\n",
    "            training_parameters.cnn_parameters.input_width = env.width\n",
    "            training_parameters.cnn_parameters.num_input_channels = (\n",
    "                env.num_input_channels\n",
    "            )\n",
    "        else:\n",
    "            assert (\n",
    "                training_parameters.cnn_parameters is None\n",
    "            ), \"Extra CNN parameters for non-image input\"\n",
    "        trainer_params = DiscreteActionModelParameters(\n",
    "            actions=env.actions, rl=rl_parameters, training=training_parameters\n",
    "        )\n",
    "        trainer = DQNTrainer(trainer_params, env.normalization, use_gpu)\n",
    "\n",
    "    elif model_type == ModelType.DISCRETE_ACTION.value:\n",
    "        with core.DeviceScope(c2_device):\n",
    "            training_settings = params[\"training\"]\n",
    "            training_parameters = TrainingParameters(**training_settings)\n",
    "            if env.img:\n",
    "                assert (\n",
    "                    training_parameters.cnn_parameters is not None\n",
    "                ), \"Missing CNN parameters for image input\"\n",
    "                training_parameters.cnn_parameters = CNNParameters(\n",
    "                    **training_settings[\"cnn_parameters\"]\n",
    "                )\n",
    "                training_parameters.cnn_parameters.conv_dims[0] = env.num_input_channels\n",
    "                training_parameters.cnn_parameters.input_height = env.height\n",
    "                training_parameters.cnn_parameters.input_width = env.width\n",
    "                training_parameters.cnn_parameters.num_input_channels = (\n",
    "                    env.num_input_channels\n",
    "                )\n",
    "            else:\n",
    "                assert (\n",
    "                    training_parameters.cnn_parameters is None\n",
    "                ), \"Extra CNN parameters for non-image input\"\n",
    "            trainer_params = DiscreteActionModelParameters(\n",
    "                actions=env.actions, rl=rl_parameters, training=training_parameters\n",
    "            )\n",
    "            trainer = DiscreteActionTrainer(trainer_params, env.normalization)\n",
    "    elif model_type == ModelType.PYTORCH_PARAMETRIC_DQN.value:\n",
    "        training_settings = params[\"training\"]\n",
    "        training_parameters = TrainingParameters(**training_settings)\n",
    "        if env.img:\n",
    "            assert (\n",
    "                training_parameters.cnn_parameters is not None\n",
    "            ), \"Missing CNN parameters for image input\"\n",
    "            training_parameters.cnn_parameters = CNNParameters(\n",
    "                **training_settings[\"cnn_parameters\"]\n",
    "            )\n",
    "            training_parameters.cnn_parameters.conv_dims[0] = env.num_input_channels\n",
    "        else:\n",
    "            assert (\n",
    "                training_parameters.cnn_parameters is None\n",
    "            ), \"Extra CNN parameters for non-image input\"\n",
    "        trainer_params = ContinuousActionModelParameters(\n",
    "            rl=rl_parameters,\n",
    "            training=training_parameters,\n",
    "            knn=KnnParameters(model_type=\"DQN\"),\n",
    "        )\n",
    "        trainer = ParametricDQNTrainer(\n",
    "            trainer_params, env.normalization, env.normalization_action, use_gpu\n",
    "        )\n",
    "    elif model_type == ModelType.PARAMETRIC_ACTION.value:\n",
    "        with core.DeviceScope(c2_device):\n",
    "            training_settings = params[\"training\"]\n",
    "            training_parameters = TrainingParameters(**training_settings)\n",
    "            if env.img:\n",
    "                assert (\n",
    "                    training_parameters.cnn_parameters is not None\n",
    "                ), \"Missing CNN parameters for image input\"\n",
    "                training_parameters.cnn_parameters = CNNParameters(\n",
    "                    **training_settings[\"cnn_parameters\"]\n",
    "                )\n",
    "                training_parameters.cnn_parameters.conv_dims[0] = env.num_input_channels\n",
    "            else:\n",
    "                assert (\n",
    "                    training_parameters.cnn_parameters is None\n",
    "                ), \"Extra CNN parameters for non-image input\"\n",
    "            trainer_params = ContinuousActionModelParameters(\n",
    "                rl=rl_parameters,\n",
    "                training=training_parameters,\n",
    "                knn=KnnParameters(model_type=\"DQN\"),\n",
    "            )\n",
    "            trainer = ContinuousActionDQNTrainer(\n",
    "                trainer_params, env.normalization, env.normalization_action\n",
    "            )\n",
    "    elif model_type == ModelType.CONTINUOUS_ACTION.value:\n",
    "        training_settings = params[\"shared_training\"]\n",
    "        actor_settings = params[\"actor_training\"]\n",
    "        critic_settings = params[\"critic_training\"]\n",
    "        trainer_params = DDPGModelParameters(\n",
    "            rl=rl_parameters,\n",
    "            shared_training=DDPGTrainingParameters(**training_settings),\n",
    "            actor_training=DDPGNetworkParameters(**actor_settings),\n",
    "            critic_training=DDPGNetworkParameters(**critic_settings),\n",
    "        )\n",
    "\n",
    "        action_range_low = env.action_space.low.astype(np.float32)\n",
    "        action_range_high = env.action_space.high.astype(np.float32)\n",
    "\n",
    "        trainer = DDPGTrainer(\n",
    "            trainer_params,\n",
    "            env.normalization,\n",
    "            env.normalization_action,\n",
    "            torch.from_numpy(action_range_low).unsqueeze(dim=0),\n",
    "            torch.from_numpy(action_range_high).unsqueeze(dim=0),\n",
    "            use_gpu,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Model of type {} not supported\".format(model_type))\n",
    "\n",
    "    return run(\n",
    "        c2_device,\n",
    "        env,\n",
    "        model_type,\n",
    "        trainer,\n",
    "        \"{} test run\".format(env_type),\n",
    "        score_bar,\n",
    "        **params[\"run_details\"],\n",
    "        save_timesteps_to_dataset=save_timesteps_to_dataset,\n",
    "        start_saving_from_episode=start_saving_from_episode,\n",
    "        batch_rl_file_path=batch_rl_file_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If set, use logging level specified (debug, info, warning, error, \"\n",
    "# \"critical). Else defaults to info\n",
    "log_level=\"info\"\n",
    "#Bar for averaged tests scores\n",
    "score_bar=None\n",
    "#Path to JSON parameters file\n",
    "parameters='ml/rl/test/gym/discrete_pytorch_dqn_cartpole_v0.json'\n",
    "#If set, will use GPU with specified ID. Otherwise will use CPU.\n",
    "USE_CPU=-1\n",
    "gpu_id=USE_CPU\n",
    "#If set, save all collected samples as an RLDataset to this file\n",
    "file_path=None\n",
    "#If file_path is set, start saving episodes from this episode num\n",
    "start_saving_from_episode=0\n",
    "#If set, train in batch RL mode (policy is trained on off-policy transitions at file path).\n",
    "batch_rl_file_path=None\n",
    "#If set, save evaluation results to file\n",
    "results_file_path='res_discrete_pytorch_dqn_cartpole_v0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_level not in (\"debug\", \"info\", \"warning\", \"error\", \"critical\"):\n",
    "    raise Exception(\"Logging level {} not valid level.\".format(log_level))\n",
    "else:\n",
    "    logger.setLevel(getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(parameters, \"r\") as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RLDataset(file_path) if file_path else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running gym with params\n",
      "INFO:__main__:{'env': 'CartPole-v0', 'model_type': 'pytorch_discrete_dqn', 'max_replay_memory_size': 10000, 'rl': {'gamma': 0.99, 'target_update_rate': 0.2, 'reward_burnin': 1, 'maxq_learning': 1, 'epsilon': 0.2, 'temperature': 0.35, 'softmax_policy': 0}, 'training': {'layers': [-1, 128, 64, -1], 'activations': ['relu', 'relu', 'linear'], 'minibatch_size': 64, 'learning_rate': 0.001, 'optimizer': 'ADAM', 'lr_decay': 0.999}, 'run_details': {'num_episodes': 5001, 'max_steps': 200, 'train_every_ts': 1, 'train_after_ts': 1, 'test_every_ts': 2000, 'test_after_ts': 1, 'num_train_batches': 1, 'avg_over_num_episodes': 100}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Achieved an average reward score of 177.92 over 100 evaluations. Total episodes: 50, total timesteps: 2000.\n",
      "INFO:__main__:Achieved an average reward score of 152.78 over 100 evaluations. Total episodes: 62, total timesteps: 4000.\n",
      "INFO:__main__:Achieved an average reward score of 164.87 over 100 evaluations. Total episodes: 74, total timesteps: 6000.\n",
      "INFO:__main__:Achieved an average reward score of 174.91 over 100 evaluations. Total episodes: 86, total timesteps: 8000.\n",
      "INFO:__main__:Achieved an average reward score of 190.48 over 100 evaluations. Total episodes: 99, total timesteps: 10000.\n",
      "INFO:__main__:Achieved an average reward score of 165.3 over 100 evaluations. Total episodes: 111, total timesteps: 12000.\n",
      "INFO:__main__:Achieved an average reward score of 149.65 over 100 evaluations. Total episodes: 124, total timesteps: 14000.\n",
      "INFO:__main__:Achieved an average reward score of 162.66 over 100 evaluations. Total episodes: 135, total timesteps: 16000.\n",
      "INFO:__main__:Achieved an average reward score of 166.32 over 100 evaluations. Total episodes: 148, total timesteps: 18000.\n",
      "INFO:__main__:Achieved an average reward score of 171.94 over 100 evaluations. Total episodes: 160, total timesteps: 20000.\n",
      "INFO:__main__:Achieved an average reward score of 177.85 over 100 evaluations. Total episodes: 172, total timesteps: 22000.\n",
      "INFO:__main__:Achieved an average reward score of 182.13 over 100 evaluations. Total episodes: 184, total timesteps: 24000.\n",
      "INFO:__main__:Achieved an average reward score of 182.37 over 100 evaluations. Total episodes: 195, total timesteps: 26000.\n",
      "INFO:__main__:Achieved an average reward score of 148.84 over 100 evaluations. Total episodes: 205, total timesteps: 28000.\n",
      "INFO:__main__:Achieved an average reward score of 200.0 over 100 evaluations. Total episodes: 215, total timesteps: 30000.\n"
     ]
    }
   ],
   "source": [
    "reward_history, timestep_history, trainer, predictor = run_gym(\n",
    "        params,\n",
    "        score_bar,\n",
    "        gpu_id,\n",
    "        dataset,\n",
    "        start_saving_from_episode,\n",
    "        batch_rl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset:\n",
    "        dataset.save()\n",
    "if results_file_path:\n",
    "    write_lists_to_csv(results_file_path, reward_history, timestep_history)\n",
    "return reward_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
